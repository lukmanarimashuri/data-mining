{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Tugas Penambangan Data \u00b6 Nama : Lukman Ari Mashuri NIM : 180411100074 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab, S. Si., M. Kom. Jurusan : Teknik Informatika","title":"Home"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"Nama : Lukman Ari Mashuri NIM : 180411100074 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab, S. Si., M. Kom. Jurusan : Teknik Informatika","title":"Selamat Datang di Halaman Tugas Penambangan Data"},{"location":"Windows/","text":"Windows \u00b6 1. Rata-rata \u00b6 print ( 'jskjsklj' )","title":"**Windows**"},{"location":"Windows/#windows","text":"","title":"Windows"},{"location":"Windows/#1-rata-rata","text":"print ( 'jskjsklj' )","title":"1. Rata-rata"},{"location":"cluster/","text":"CLUSTERING Pengertian Clustering adalah metode pengelompokan data. Clustering bisa disebut sebagai suatu proses untuk mengelompokan data ke dalam beberapa cluster sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Clustering merupakan proses partisi satu set objek data ke dalam himpunan bagian yang disebut dengan cluster. Objek yang di dalam cluster memiliki kemiripan karakteristik antar satu sama lainnya dan berbeda dengan cluster yang lain. Partisi tidak dilakukan secara manual melainkan dengan suatu algoritma clustering . Oleh karena itu, clustering sangat berguna dan bisa menemukan group atau kelompokyang tidak dikenal dalam data. Clustering banyak digunakan dalam berbagai aplikasi seperti misalnya pada business inteligence, pengenalan pola citra , web search, bidang ilmu biologi, dan untuk keamanan ( security ). Di dalam business inteligence , clustering bisa mengatur banyak customer ke dalam banyaknya kelompok. Contohnya mengelompokan customer ke dalam beberapa cluster dengan kesamaan karakteristik yang kuat. Clustering juga dikenal sebagai data segmentasi karena clustering mempartisi banyak data set ke dalam banyak group berdasarkan kesamaannya. Selain itu clustering juga bisa sebagai outlier detection . Metode K-Means Clustering K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: 1. Menentukan jumlah cluster 2. Mengalokasikan data secara random ke cluster yang ada 3. Menghitung rata-rata setiap cluster dari data yang tergabung sebelumnya 4. Mengalokasikan kembali semua data ke cluster tersebut 5. Mengulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah *crisp* atau *fuzzy*. Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa *k-means clustering* mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Rumus K-Means $$ d(x,y)=|x-y|= \\sqrt{\\sum _ { i = 1 } ^ { n } (x _ { i }-y_{i})^2} $$ Implementasikan K-Means Menggunakan Python from time import time import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.cluster import KMeans from sklearn.datasets import load_digits from sklearn.decomposition import PCA from sklearn.preprocessing import scale np . random . seed ( 42 ) digits = load_digits () data = scale ( digits . data ) n_samples , n_features = data . shape n_digits = len ( np . unique ( digits . target )) labels = digits . target sample_size = 300 print ( \"n_digits: %d , \\t n_samples %d , \\t n_features %d \" % ( n_digits , n_samples , n_features )) print ( 82 * '_' ) print ( 'init \\t\\t time \\t inertia \\t homo \\t compl \\t v-meas \\t ARI \\t AMI \\t silhouette' ) def bench_k_means ( estimator , name , data ): t0 = time () estimator . fit ( data ) print ( ' %-9s \\t %.2f s \\t %i \\t %.3f \\t %.3f \\t %.3f \\t %.3f \\t %.3f \\t %.3f ' % ( name , ( time () - t0 ), estimator . inertia_ , metrics . homogeneity_score ( labels , estimator . labels_ ), metrics . completeness_score ( labels , estimator . labels_ ), metrics . v_measure_score ( labels , estimator . labels_ ), metrics . adjusted_rand_score ( labels , estimator . labels_ ), metrics . adjusted_mutual_info_score ( labels , estimator . labels_ , average_method = 'arithmetic' ), metrics . silhouette_score ( data , estimator . labels_ , metric = 'euclidean' , sample_size = sample_size ))) bench_k_means ( KMeans ( init = 'k-means++' , n_clusters = n_digits , n_init = 10 ), name = \"k-means++\" , data = data ) bench_k_means ( KMeans ( init = 'random' , n_clusters = n_digits , n_init = 10 ), name = \"random\" , data = data ) # in this case the seeding of the centers is deterministic, hence we run the # kmeans algorithm only once with n_init=1 pca = PCA ( n_components = n_digits ) . fit ( data ) bench_k_means ( KMeans ( init = pca . components_ , n_clusters = n_digits , n_init = 1 ), name = \"PCA-based\" , data = data ) print ( 82 * '_' ) # Visualize the results on PCA-reduced data reduced_data = PCA ( n_components = 2 ) . fit_transform ( data ) kmeans = KMeans ( init = 'k-means++' , n_clusters = n_digits , n_init = 10 ) kmeans . fit ( reduced_data ) # Step size of the mesh. Decrease to increase the quality of the VQ. h = . 02 # point in the mesh [x_min, x_max]x[y_min, y_max]. # Plot the decision boundary. For that, we will assign a color to each x_min , x_max = reduced_data [:, 0 ] . min () - 1 , reduced_data [:, 0 ] . max () + 1 y_min , y_max = reduced_data [:, 1 ] . min () - 1 , reduced_data [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) # Obtain labels for each point in mesh. Use last trained model. Z = kmeans . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure ( 1 ) plt . clf () plt . imshow ( Z , interpolation = 'nearest' , extent = ( xx . min (), xx . max (), yy . min (), yy . max ()), cmap = plt . cm . Paired , aspect = 'auto' , origin = 'lower' ) plt . plot ( reduced_data [:, 0 ], reduced_data [:, 1 ], 'k.' , markersize = 2 ) # Plot the centroids as a white X centroids = kmeans . cluster_centers_ plt . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = 'x' , s = 169 , linewidths = 3 , color = 'w' , zorder = 10 ) plt . title ( 'K-means clustering on the digits dataset (PCA-reduced data) \\n ' 'Centroids are marked with white cross' ) plt . xlim ( x_min , x_max ) plt . ylim ( y_min , y_max ) plt . xticks (()) plt . yticks (()) plt . show () n_digits : 10 , n_samples 1797 , n_features 64 ------ init time inertia homo compl v - meas ARI AMI silhouette k - means ++ 0.19 s 69432 0.602 0.650 0.625 0.465 0.621 0.146 random 0.19 s 69694 0.669 0.710 0.689 0.553 0.686 0.147 PCA - based 0.04 s 70804 0.671 0.698 0.684 0.561 0.681 0.118 ![output_1_1](C:\\Users\\LAM\\Desktop\\output_1_1.png) K-Means dengan Tiga Cluster import numpy as np import matplotlib.pyplot as plt # Though the following import is not directly being used, it is required # for 3D projection to work from mpl_toolkits.mplot3d import Axes3D from sklearn.cluster import KMeans from sklearn import datasets np . random . seed ( 5 ) iris = datasets . load_iris () X = iris . data y = iris . target estimators = [( 'k_means_iris_8' , KMeans ( n_clusters = 8 )), ( 'k_means_iris_3' , KMeans ( n_clusters = 3 )), ( 'k_means_iris_bad_init' , KMeans ( n_clusters = 3 , n_init = 1 , init = 'random' ))] fignum = 1 titles = [ '8 clusters' , '3 clusters' , '3 clusters, bad initialization' ] for name , est in estimators : fig = plt . figure ( fignum , figsize = ( 4 , 3 )) ax = Axes3D ( fig , rect = [ 0 , 0 , . 95 , 1 ], elev = 48 , azim = 134 ) est . fit ( X ) labels = est . labels_ ax . scatter ( X [:, 3 ], X [:, 0 ], X [:, 2 ], c = labels . astype ( np . float ), edgecolor = 'k' ) ax . w_xaxis . set_ticklabels ([]) ax . w_yaxis . set_ticklabels ([]) ax . w_zaxis . set_ticklabels ([]) ax . set_xlabel ( 'Petal width' ) ax . set_ylabel ( 'Sepal length' ) ax . set_zlabel ( 'Petal length' ) ax . set_title ( titles [ fignum - 1 ]) ax . dist = 12 fignum = fignum + 1 # Plot the ground truth fig = plt . figure ( fignum , figsize = ( 4 , 3 )) ax = Axes3D ( fig , rect = [ 0 , 0 , . 95 , 1 ], elev = 48 , azim = 134 ) for name , label in [( 'Setosa' , 0 ), ( 'Versicolour' , 1 ), ( 'Virginica' , 2 )]: ax . text3D ( X [ y == label , 3 ] . mean (), X [ y == label , 0 ] . mean (), X [ y == label , 2 ] . mean () + 2 , name , horizontalalignment = 'center' , bbox = dict ( alpha =. 2 , edgecolor = 'w' , facecolor = 'w' )) # Reorder the labels to have colors matching the cluster results y = np . choose ( y , [ 1 , 2 , 0 ]) . astype ( np . float ) ax . scatter ( X [:, 3 ], X [:, 0 ], X [:, 2 ], c = y , edgecolor = 'k' ) ax . w_xaxis . set_ticklabels ([]) ax . w_yaxis . set_ticklabels ([]) ax . w_zaxis . set_ticklabels ([]) ax . set_xlabel ( 'Petal width' ) ax . set_ylabel ( 'Sepal length' ) ax . set_zlabel ( 'Petal length' ) ax . set_title ( 'Ground Truth' ) ax . dist = 12 fig . show () ## Metode K-Modes K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster ###### Implementasi K-Modes Dengan Python menggunakan Random Kategorikal Data import numpy as np from kmodes.kmodes import KModes # random categorical data data = np . random . choice ( 20 , ( 100 , 10 )) km = KModes ( n_clusters = 4 , init = 'Huang' , n_init = 5 , verbose = 1 ) clusters = km . fit_predict ( data ) # Print the cluster centroids print ( km . cluster_centroids_ ) Init : initializing centroids Init : initializing clusters Starting iterations ... Run 1 , iteration : 1 / 100 , moves : 28 , cost : 793.0 Run 1 , iteration : 2 / 100 , moves : 1 , cost : 793.0 Init : initializing centroids Init : initializing clusters Starting iterations ... Run 2 , iteration : 1 / 100 , moves : 28 , cost : 791.0 Run 2 , iteration : 2 / 100 , moves : 4 , cost : 789.0 Run 2 , iteration : 3 / 100 , moves : 3 , cost : 789.0 Init : initializing centroids Init : initializing clusters Starting iterations ... Run 3 , iteration : 1 / 100 , moves : 20 , cost : 797.0 Run 3 , iteration : 2 / 100 , moves : 7 , cost : 792.0 Run 3 , iteration : 3 / 100 , moves : 3 , cost : 792.0 Init : initializing centroids Init : initializing clusters Starting iterations ... Run 4 , iteration : 1 / 100 , moves : 21 , cost : 799.0 Run 4 , iteration : 2 / 100 , moves : 6 , cost : 798.0 Run 4 , iteration : 3 / 100 , moves : 0 , cost : 798.0 Init : initializing centroids Init : initializing clusters Starting iterations ... Run 5 , iteration : 1 / 100 , moves : 18 , cost : 795.0 Run 5 , iteration : 2 / 100 , moves : 6 , cost : 795.0 Best run was number 2 [[ 14 8 0 18 3 7 0 1 16 3 ] [ 7 1 12 4 18 16 5 17 6 2 ] [ 9 17 3 2 11 5 11 0 11 1 ] [ 8 13 8 3 9 0 2 12 6 9 ]] ### Metode K-Prototype Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek 1. Tahap 1: \u200b Tentukan K dengan inisial kluster z1, z2, ...,zk secara acak dari n buah titik {x1, x2,...,xn} 2. Tahap 2 \u200b Hitung jarak seluruh data point pada datas et terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memilik i jarak prototype terdekat dengan object yang diukur. 3. Tahap 3 \u200b Hitung titik pusat cluster yang baru setela h semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru 4. Tahap 4 \u200b jika titik pusat cluster tidak berubah ata u sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih be rubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. K- Prototype ini adalah Gabungan data yang ada numerik (data digit) seperti k-Means dan ada data kategorikal dari k-Modes Di bawah ini diberikan adalah kategorisasi set data di atas dengan menggunakan algoritma k-prototype import numpy as np from kmodes.kprototypes import KPrototypes import matplotlib.pyplot as plt from matplotlib import style style . use ( \"ggplot\" ) colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] #Data points with their publisher name,category score, category name, place name syms = np . genfromtxt ( 'travel.csv' , dtype = str , delimiter = ',' )[:, 1 ] X = np . genfromtxt ( 'travel.csv' , dtype = object , delimiter = ',' )[:, 2 :] X [:, 0 ] = X [:, 0 ] . astype ( float ) kproto = KPrototypes ( n_clusters = 15 , init = 'Cao' , verbose = 2 ) clusters = kproto . fit_predict ( X , categorical = [ 1 , 2 ]) # Print cluster centroids of the trained model. print ( kproto . cluster_centroids_ ) # Print training statistics print ( kproto . cost_ ) print ( kproto . n_iter_ ) for s , c in zip ( syms , clusters ): print ( \"Result: {}, cluster:{}\" . format ( s , c )) # Plot the results for i in set ( kproto . labels_ ): index = kproto . labels_ == i plt . plot ( X [ index , 0 ], X [ index , 1 ], 'o' ) plt . suptitle ( 'Data points categorized with category score' , fontsize = 18 ) plt . xlabel ( 'Category Score' , fontsize = 16 ) plt . ylabel ( 'Category Type' , fontsize = 16 ) plt . show () # Clustered result fig1 , ax3 = plt . subplots () scatter = ax3 . scatter ( syms , clusters , c = clusters , s = 50 ) ax3 . set_xlabel ( 'Data points' ) ax3 . set_ylabel ( 'Cluster' ) plt . colorbar ( scatter ) ax3 . set_title ( 'Data points classifed according to known centers' ) plt . show () result = zip ( syms , kproto . labels_ ) sortedR = sorted ( result , key = lambda x : x [ 1 ]) print ( sortedR ) 240 , Ransika Fernando , 0.59375 , plant , No Data 240 , Ransika Fernando , 0.04296875 , outdoor_ , No Data 240 , Ransika Fernando , 0.26953125 , outdoor_road , No Data 241 , Sachini Jagodaarachchi , 0.98046875 , outdoor_mountain , Manigala Mountain 242 , Chathuri Senanayake , 0.96484375 , outdoor_mountain , Adara Kanda 242 , Chathuri Senanayake , 0.1953125 , building_ , No Data 242 , Chathuri Senanayake , 0.00390625 , outdoor_ , No Data 242 , Chathuri Senanayake , 0.23046875 , building_ , Kuwait 242 , Chathuri Senanayake , 0.2578125 , building_street , Kuwait 242 , Chathuri Senanayake , 0.015625 , outdoor_ , Kuwait 243 , Nilantha Premakumara , 0.9453125 , sky_sun , No Data 243 , Nilantha Premakumara , 0.75 , outdoor_mountain , No Data 244 , Chathuri Senanayake , 0.00390625 , outdoor_ , Trincomalee 244 , Chathuri Senanayake , 0.6328125 , outdoor_oceanbeach , Trincomalee 245 , Surangani Bandara , 0.7734375 , plant_tree , No Data 246 , Hasitha Lakmal , 0.4140625 , people_many , No Data 246 , Hasitha Lakmal , 0.0078125 , outdoor_ , No Data 247 , Pradeep Kalansooriya , 0.40234375 , building_ , No Data 247 , Pradeep Kalansooriya , 0.0078125 , outdoor_ , No Data 248 , Dilini Wijesinghe , 0.07421875 , outdoor_ , Victoria Dam 248 , Dilini Wijesinghe , 0.0078125 , others_ , Victoria Dam 249 , Chiranthi Vinghghani , 0.015625 , outdoor_ , No Data 249 , Chiranthi Vinghghani , 0.6484375 , outdoor_waterside , No Data 250 , Janindu Praneeth Weerawarnakula , 0.671875 , outdoor_oceanbeach , Galle Fort 251 , Chathurangi Shyalika , 0.00390625 , outdoor_ , No Data 252 , Chathurangi Shyalika , 0.9296875 , trans_trainstation , No Data 253 , Surangani Bandara , 0.625 , outdoor_field , No Data 253 , Surangani Bandara , 0.01171875 , outdoor_ , No Data 254 , Surangani Bandara , 0.99609375 , sky_object , No Data 255 , Chathurangi Shyalika , 0.00390625 , outdoor_ , No Data 256 , Chathurangi Shyalika , 0.33984375 , outdoor_field , No Data","title":"Clustering"},{"location":"decision_tree/","text":"Decision Tree \u00b6 Decision Tree \u00b6 Decisioin tree adalah alat pendukung keputusan yang menggunakan model keputusan seperti pohon. Decisioin tree (pohon keputusan) biasanya digunakan dalam riset operasi, khususnya dalam analisis keputusan, untuk membantu mengidentifikasi strategi yang paling memungkinkan untuk mencapai tujuan, dan juga merupakan alat yang populer dalam pembelajaran mesin. Dalam ilmu komputer, pembelajaran Decision tree sebagai model prediktif untuk melakukan pengamatan tentang item (diwakili di cabang-cabang) ke kesimpulan tentang nilai target item (diwakili dalam daun). Decision tree ini adalah salah satu pendekatan pemodelan prediktif yang digunakan dalam statistik, penambangan data, dan pembelajaran mesin. Dalam struktur decision tree, daun mewakili label kelas dan cabang mewakili konjungsi fitur yang mengarah ke label kelas tersebut. Decision tree ini di mana variabel target dapat mengambil nilai kontinu yang disebut pohon regresi. Entropy \u00b6 Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ Keterangan : S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S Gain \u00b6 Gain adalah ukuran efektifitas suatu atribut dalam mengklasifikasikan data, gain digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai information Gain terbesar yang dipilih. $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ Keterangan : S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S GINI Index \u00b6 Dalam penerapan GINI index untuk data berskala continuous , terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode brute-force dan metode midpoints . Script \u00b6 # menentukan value atau jenis pada atribut def banyak_elemen ( kolom , data ): kelas = [] for i in range ( len ( data )): if data . values . tolist ()[ i ][ kolom ] not in kelas : kelas . append ( data . values . tolist ()[ i ][ kolom ]) return kelas kelas = banyak_elemen ( df . shape [ 1 ] - 1 , df ) outlook = banyak_elemen ( df . shape [ 1 ] - 5 , df ) temp = banyak_elemen ( df . shape [ 1 ] - 4 , df ) humidity = banyak_elemen ( df . shape [ 1 ] - 3 , df ) windy = banyak_elemen ( df . shape [ 1 ] - 2 , df ) print ( kelas , outlook , temp , humidity , windy ) ` [ 'no' , 'yes' ] [ 'sunny' , 'overcast' , 'rainy' ] [ 'hot' , 'mild' , 'cool' ] [ 'high' , 'normal' ] [ False , True ] # menentukan count value pada Kelas #Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class def countvKelas ( kelas , kolomKelas , data ): hasil = [] for x in range ( len ( kelas )): hasil . append ( 0 ) for i in range ( len ( data )): for j in range ( len ( kelas )): if data . values . tolist ()[ i ][ kolomKelas ] == kelas [ j ]: hasil [ j ] += 1 return hasil pKelas = countvKelas ( kelas , df . shape [ 1 ] - 1 , df ) pKelas [ 5 , 9 ] # menentukan nilai entropy target # Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture def entropy ( T ): hasil = 0 jumlah = 0 for y in T : jumlah += y for z in range ( len ( T )): if jumlah != 0 : T [ z ] = T [ z ] / jumlah for i in T : if i != 0 : hasil -= i * math . log ( i , 2 ) return hasil def e_list ( atribut , n ): temp = [] tx = t_list ( atribut , n ) for i in range ( len ( atribut )): ent = entropy ( tx [ i ]) temp . append ( ent ) return temp tOutlook = t_list ( outlook , 5 ) tTemp = t_list ( temp , 4 ) tHum = t_list ( humidity , 3 ) tWin = t_list ( windy , 2 ) print ( \"Sunny, Overcast, Rainy\" , eOutlook ) print ( \"Hot, Mild, Cold\" , eTemp ) print ( \"High, Normal\" , eHum ) print ( \"False, True\" , eWin ) Sunny , Overcast , Rainy [ 0.9709505944546686 , 0.0 , 0.9709505944546686 ] Hot , Mild , Cold [ 1.0 , 0.9182958340544896 , 0.8112781244591328 ] High , Normal [ 0.9852281360342516 , 0.5916727785823275 ] False , True [ 0.8112781244591328 , 1.0 ] Berikut adalah contoh data yang akan dirubah menjadi decision tree: 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas, diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ Kemudian menghitung gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] \u200b = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) \u200b =1-(10/20 x 0,970951)+(10/20 x 0,970951) \u200b =1-(0,4485475+0,4485475) \u200b =1-0,970951 \u200b =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] \u200b = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) \u200b =1-(0,162256+0+0,217426) \u200b =1-0,379681 \u200b =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] \u200b = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) \u200b =1-(0,242738+0,34483+0,2+0,2) \u200b =1-0,987567 \u200b =0,012433 Referensi \u00b6 https://en.wikipedia.org/wiki/Decision_tree_learning http://dinus.ac.id/repository/docs/ajar/5DecTreeClass.pdf","title":"Decision Tree"},{"location":"decision_tree/#decision-tree","text":"","title":"Decision Tree"},{"location":"decision_tree/#decision-tree_1","text":"Decisioin tree adalah alat pendukung keputusan yang menggunakan model keputusan seperti pohon. Decisioin tree (pohon keputusan) biasanya digunakan dalam riset operasi, khususnya dalam analisis keputusan, untuk membantu mengidentifikasi strategi yang paling memungkinkan untuk mencapai tujuan, dan juga merupakan alat yang populer dalam pembelajaran mesin. Dalam ilmu komputer, pembelajaran Decision tree sebagai model prediktif untuk melakukan pengamatan tentang item (diwakili di cabang-cabang) ke kesimpulan tentang nilai target item (diwakili dalam daun). Decision tree ini adalah salah satu pendekatan pemodelan prediktif yang digunakan dalam statistik, penambangan data, dan pembelajaran mesin. Dalam struktur decision tree, daun mewakili label kelas dan cabang mewakili konjungsi fitur yang mengarah ke label kelas tersebut. Decision tree ini di mana variabel target dapat mengambil nilai kontinu yang disebut pohon regresi.","title":"Decision Tree"},{"location":"decision_tree/#entropy","text":"Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ Keterangan : S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S","title":"Entropy"},{"location":"decision_tree/#gain","text":"Gain adalah ukuran efektifitas suatu atribut dalam mengklasifikasikan data, gain digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai information Gain terbesar yang dipilih. $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ Keterangan : S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S","title":"Gain"},{"location":"decision_tree/#gini-index","text":"Dalam penerapan GINI index untuk data berskala continuous , terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode brute-force dan metode midpoints .","title":"GINI Index"},{"location":"decision_tree/#script","text":"# menentukan value atau jenis pada atribut def banyak_elemen ( kolom , data ): kelas = [] for i in range ( len ( data )): if data . values . tolist ()[ i ][ kolom ] not in kelas : kelas . append ( data . values . tolist ()[ i ][ kolom ]) return kelas kelas = banyak_elemen ( df . shape [ 1 ] - 1 , df ) outlook = banyak_elemen ( df . shape [ 1 ] - 5 , df ) temp = banyak_elemen ( df . shape [ 1 ] - 4 , df ) humidity = banyak_elemen ( df . shape [ 1 ] - 3 , df ) windy = banyak_elemen ( df . shape [ 1 ] - 2 , df ) print ( kelas , outlook , temp , humidity , windy ) ` [ 'no' , 'yes' ] [ 'sunny' , 'overcast' , 'rainy' ] [ 'hot' , 'mild' , 'cool' ] [ 'high' , 'normal' ] [ False , True ] # menentukan count value pada Kelas #Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class def countvKelas ( kelas , kolomKelas , data ): hasil = [] for x in range ( len ( kelas )): hasil . append ( 0 ) for i in range ( len ( data )): for j in range ( len ( kelas )): if data . values . tolist ()[ i ][ kolomKelas ] == kelas [ j ]: hasil [ j ] += 1 return hasil pKelas = countvKelas ( kelas , df . shape [ 1 ] - 1 , df ) pKelas [ 5 , 9 ] # menentukan nilai entropy target # Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture def entropy ( T ): hasil = 0 jumlah = 0 for y in T : jumlah += y for z in range ( len ( T )): if jumlah != 0 : T [ z ] = T [ z ] / jumlah for i in T : if i != 0 : hasil -= i * math . log ( i , 2 ) return hasil def e_list ( atribut , n ): temp = [] tx = t_list ( atribut , n ) for i in range ( len ( atribut )): ent = entropy ( tx [ i ]) temp . append ( ent ) return temp tOutlook = t_list ( outlook , 5 ) tTemp = t_list ( temp , 4 ) tHum = t_list ( humidity , 3 ) tWin = t_list ( windy , 2 ) print ( \"Sunny, Overcast, Rainy\" , eOutlook ) print ( \"Hot, Mild, Cold\" , eTemp ) print ( \"High, Normal\" , eHum ) print ( \"False, True\" , eWin ) Sunny , Overcast , Rainy [ 0.9709505944546686 , 0.0 , 0.9709505944546686 ] Hot , Mild , Cold [ 1.0 , 0.9182958340544896 , 0.8112781244591328 ] High , Normal [ 0.9852281360342516 , 0.5916727785823275 ] False , True [ 0.8112781244591328 , 1.0 ] Berikut adalah contoh data yang akan dirubah menjadi decision tree: 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari entropy(s) dari kolom class di atas, diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ Kemudian menghitung gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] \u200b = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) \u200b =1-(10/20 x 0,970951)+(10/20 x 0,970951) \u200b =1-(0,4485475+0,4485475) \u200b =1-0,970951 \u200b =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] \u200b = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) \u200b =1-(0,162256+0+0,217426) \u200b =1-0,379681 \u200b =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] \u200b = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) \u200b =1-(0,242738+0,34483+0,2+0,2) \u200b =1-0,987567 \u200b =0,012433","title":"Script"},{"location":"decision_tree/#referensi","text":"https://en.wikipedia.org/wiki/Decision_tree_learning http://dinus.ac.id/repository/docs/ajar/5DecTreeClass.pdf","title":"Referensi"},{"location":"deretmclaurin/","text":"Deret McLaurin Deret McLaurin adalah suatu fungsi f(x) yang memiliki turunan f'(x),f''(x), f'''(x) dan seterusnya yang kontinu dalam interval i dengan a, x i maka untuk x disekitar a yaitu |x-a|<R,f(x) dapat diexpansi kedalam deret Taylor, dalam suatu kasus tertentu jika a=0, maka disebut deret McLaurin atau sering disebut deret Taylor baku. Definisi : \" style=\"zoom:100%;\" /> Atau bisa dinyatakan dengan: \" style=\"zoom:100%;\" /> Menghitung nilai e^3x untuk nilai x=4, kemudian mengexpensikan hingga selisih kurang dari nilai error yang ditentukan yaitu e < 0,001. Fungsi awal exponen: \" style=\"zoom:100%;\" /> Dapat juga didefinisikan dengan rumus : \" style=\"zoom:100%;\" /> Derer turunan: \" style=\"zoom:100%;\" /> Berikut adalah penyelesaian untuk mencari nilai expansi : \" style=\"zoom:100%;\" /> nilai turunan pada tabel dimasukkan kedalam rumus sehingga didapatkan seperti ini : \" style=\"zoom:100%;\" /> kemudian, nilai x diganti dengan 4 : \" style=\"zoom:100%;\" /> perhitungan diatas akan terus berulang hingga nilai selisih mendekati nilai error yang ditentukan yaitu kurang dari 0,001 \" style=\"zoom:100%;\" /> Listing Program import math error = 0.001 def f ( x ): f_turunan = 1 current = i = 0 iteration = True while iteration : old = current current += ( f_turunan * ( x ** i )) / math . factorial ( i ) print ( 'f ke-' , i , '=' , current , 'Ea=' , current - old ) if current - old < error : iteration = False else : f_turunan *= 3 i += 1 f ( 4 ) f ke- 0 = 1.0 Ea= 1.0 f ke- 1 = 13.0 Ea= 12.0 f ke- 2 = 85.0 Ea= 72.0 f ke- 3 = 373.0 Ea= 288.0 f ke- 4 = 1237.0 Ea= 864.0 f ke- 5 = 3310.6 Ea= 2073.6 f ke- 6 = 7457.799999999999 Ea= 4147.199999999999 f ke- 7 = 14567.285714285714 Ea= 7109.4857142857145 f ke- 8 = 25231.514285714286 Ea= 10664.228571428572 f ke- 9 = 39450.485714285714 Ea= 14218.971428571429 f ke- 10 = 56513.25142857143 Ea= 17062.765714285713 f ke- 11 = 75127.17766233766 Ea= 18613.926233766237 f ke- 12 = 93741.1038961039 Ea= 18613.926233766237 f ke- 13 = 110923.18965034965 Ea= 17182.085754245752 f ke- 14 = 125650.69172541745 Ea= 14727.502075067794 f ke- 15 = 137432.69338547168 Ea= 11782.00166005423 f ke- 16 = 146269.19463051236 Ea= 8836.50124504068 f ke- 17 = 152506.7249211293 Ea= 6237.530290616938 f ke- 18 = 156665.07844820726 Ea= 4158.3535270779685 f ke- 19 = 159291.4069916249 Ea= 2626.3285434176505 f ke- 20 = 160867.20411767552 Ea= 1575.797126050602 f ke- 21 = 161767.65961827585 Ea= 900.4555006003357 f ke- 22 = 162258.81716405787 Ea= 491.1575457820145 f ke- 23 = 162515.07327490064 Ea= 256.25611084277625 f ke- 24 = 162643.20133032204 Ea= 128.12805542140268 f ke- 25 = 162704.7027969243 Ea= 61.501466602261644 f ke- 26 = 162733.08808920227 Ea= 28.385292277962435 f ke- 27 = 162745.70377465914 Ea= 12.61568545686896 f ke- 28 = 162751.1104969978 Ea= 5.406722338666441 f ke- 29 = 162753.3477614138 Ea= 2.237264416005928 f ke- 30 = 162754.2426671802 Ea= 0.8949057663849089 f ke- 31 = 162754.58908231556 Ea= 0.34641513536917046 f ke- 32 = 162754.71898799133 Ea= 0.12990567577071488 f ke- 33 = 162754.7662264189 Ea= 0.04723842756357044 f ke- 34 = 162754.7828988051 Ea= 0.016672386205755174 f ke- 35 = 162754.7886150518 Ea= 0.005716246698284522 f ke- 36 = 162754.79052046736 Ea= 0.0019054155563935637 f ke- 37 = 162754.79113843996 Ea= 0.0006179726042319089 proses perulangan akan stop di iterasi ke-37 karena selisih dari expansi yang dihasilkan mendekati nilai error yang ditentukan yaitu e < 0,001.","title":"Deret McLaurin"},{"location":"deskriptif/","text":"Statistik Deskriptif \u00b6 Pengertian \u00b6 Statistik deskriptif adalah metode-metode pengumpulan dan penyajian data agar dapat memberikan suatu informasi yang berguna Statistik deskriptif hanya memberikan informasi mengenai data yang telah dimiliki dan menyajikan data dalam bentuk tabel diagram grafik atau dalam bentuk lainnya dalam uraian-uraian yang singkat dan terbatas. Tipe Statistik Deskriptif \u00b6 Mean (Rata-rata) \u00b6 Mean adalah rata-rata dari kumpulan angka, secara khusus, jumlah nilai dibagi dengan banyaknya angka. misal ada sebuah data, maka untuk mencari mean dapat dihitung dengan rumus berikut ini: $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Keterangan: * x bar = x rata-rata * x = data ke n * n = banyaknya data Median \u00b6 Median adalah nilai pemisah bagian tengah dari urutan sebuah data. Median disimbolkan dengan Me . nilali _Quartile__2__ berbeda cara perhitungannya, yakni tergantung banyak data tersebut ganjil atau genap. berikut adalah rumus untuk menghitung median: $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Keterangan: Me = Median dari kelompok data n = banyak data Modus \u00b6 Modus adalah suatu nilai yang paling sering muncul dalam suatu data. Modus berguna untuk mengetahui tingkat frekuensi terjadinya suatu peristiwa. jika dalam suatu data ada dua nilai dengan frekuensi tertinggi, maka itu disebut bimodal, jika ada tiga disebut trimodal, dan jika ada banyak nilai dengan frekuensi tertinggi maka disebut multimodal. berikut adalah rumus untuk mencari modus dalam sebuah himpunan angka: $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Keterangan: Mo = Modus Tb = tepi bawah b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 adalah mutlak (selalu positif) Varians \u00b6 varians adalah ukuran seberapa jauh suatu kumpulan bilangan tersebar, varian merupakan jumlah kuadrat semua deviasi nilai-nilai terhadap rata-rata. berikut adalah rumus untuk mencari nilai varian dari suatu himpunan data: $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan: x = rata-rata Xi = rata-rata dari semua titik data n = banyak data Standar Deviasi \u00b6 Standar deviasi adalah nilai yang digunakan untuk menentukan sebaran data dalam sampel, serta seberapa dekat titik data individu ke rata-rata nilai sampel. Standar deviasi dapat dengan mudah dihitung dengan hanya mengakar kuadratkan nilai varians. Jika titik data lebih rendah dari rata-rata maka semakin tinggi standar deviasinya. Untuk menghitung standar deviasi dapat menggunakan rumus berikut: $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Skewness \u00b6 Skewness ( kemiringan ) mengacu pada distorsi atau asimetri dalam kurva lonceng simetris, atau distribusi normal dalam suatu set data. Skewness merupakan bentuk ketidaksimetrisan suatu distribusi data. Skewness juga adaalah angka yang menujukkan ketidak miringan atau kemiringan suatu data. berikut adalah rumus untuk mencari skewness: $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Keterangan: Xi = titik data\\ x = rata-rata n = jumlah titik distribusi o = standar deviasi Quartile \u00b6 Quartile adalah jenis Quantile. Quartile pertama (Q1) didefinisikan sebagai angka tengah antara angka terkecil dan median dari kumpulan data. Kuartil kedua (Q2) adalah median data. Kuartil ketiga (Q3) adalah nilai tengah antara median dan nilai tertinggi dari kumpulan data. Simpelnya, quantile ialah nilai yang dibagi 25%. berikut adalah rumus quantile: $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2= (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Keterangan: Q = nialai quarter n = banyak data Penerapan Statistik Deskrtiptif dalam Python \u00b6 Alat dan Bahan: \u00b6 buatlah data dengan random di excel terleih dahulu, caranya dengan menggunakan formula =RANDBETWEEN(batas_bawah;batas_atas) . kemudian copast hasil tersebut sebagai values . Setelah itu save as .csv . kita menggunakan library python yakni, pandas dan scipy. Langkah-langkah: \u00b6 Pertama \u00b6 Mengimport library yang telah disiapkan tadi, yakni scipy dan pandas import pandas as pd from scipy import stats Kedua \u00b6 Memuat data .csv yang telah dibuat df = pd . read_csv ( 'data_random.csv' , sep = ';' ) Ketiga \u00b6 Memuat penyimpanan data untuk disimpan kemudian untuk ditampilkan. Kemudian menghitung data yang diambil dari bebrapa kolom dari data file .csv dengan itersi, dan menghitung dengan cara yang telah disediakan di libary pandas. Kemudian visualisasikan data trsebut. data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes stats Tinggi Badan Berat Badan Usia Lingkar Badan Min 140 40 20 70 Max 190 70 50 100 Mean 164.882 54.72 34.832 85.228 Standard Deviasi 15.18 8.96 9.3 8.8 Variasi 230.35 80.27 86.4 77.42 Skewnes -0 0.1 0.08 -0.07 Quantile 1 151 47 27 78 Quantile 2 165 54 34 85 Quantile 3 179 63 43.25 93 Median 165 54 34 85 Modus 142 50 28 93 Referensi \u00b6 https://id.wikipedia.org/wiki/Statistika_deskriptif https://www.investopedia.com/terms/s/skewness.asp https://rumusrumus.com/standar-deviasi/ https://statmat.id/pengertian-statistik-deskriptif-dan-statistik-inferensia/ https://www.asikbelajar.com/pengertian-modus-mode/ https://en.wikipedia.org/wiki/Median https://en.wikipedia.org/wiki/Mean","title":"Statistik Deskriptif"},{"location":"deskriptif/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"deskriptif/#pengertian","text":"Statistik deskriptif adalah metode-metode pengumpulan dan penyajian data agar dapat memberikan suatu informasi yang berguna Statistik deskriptif hanya memberikan informasi mengenai data yang telah dimiliki dan menyajikan data dalam bentuk tabel diagram grafik atau dalam bentuk lainnya dalam uraian-uraian yang singkat dan terbatas.","title":"Pengertian"},{"location":"deskriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif"},{"location":"deskriptif/#mean-rata-rata","text":"Mean adalah rata-rata dari kumpulan angka, secara khusus, jumlah nilai dibagi dengan banyaknya angka. misal ada sebuah data, maka untuk mencari mean dapat dihitung dengan rumus berikut ini: $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Keterangan: * x bar = x rata-rata * x = data ke n * n = banyaknya data","title":"Mean (Rata-rata)"},{"location":"deskriptif/#median","text":"Median adalah nilai pemisah bagian tengah dari urutan sebuah data. Median disimbolkan dengan Me . nilali _Quartile__2__ berbeda cara perhitungannya, yakni tergantung banyak data tersebut ganjil atau genap. berikut adalah rumus untuk menghitung median: $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Keterangan: Me = Median dari kelompok data n = banyak data","title":"Median"},{"location":"deskriptif/#modus","text":"Modus adalah suatu nilai yang paling sering muncul dalam suatu data. Modus berguna untuk mengetahui tingkat frekuensi terjadinya suatu peristiwa. jika dalam suatu data ada dua nilai dengan frekuensi tertinggi, maka itu disebut bimodal, jika ada tiga disebut trimodal, dan jika ada banyak nilai dengan frekuensi tertinggi maka disebut multimodal. berikut adalah rumus untuk mencari modus dalam sebuah himpunan angka: $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Keterangan: Mo = Modus Tb = tepi bawah b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 adalah mutlak (selalu positif)","title":"Modus"},{"location":"deskriptif/#varians","text":"varians adalah ukuran seberapa jauh suatu kumpulan bilangan tersebar, varian merupakan jumlah kuadrat semua deviasi nilai-nilai terhadap rata-rata. berikut adalah rumus untuk mencari nilai varian dari suatu himpunan data: $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan: x = rata-rata Xi = rata-rata dari semua titik data n = banyak data","title":"Varians"},{"location":"deskriptif/#standar-deviasi","text":"Standar deviasi adalah nilai yang digunakan untuk menentukan sebaran data dalam sampel, serta seberapa dekat titik data individu ke rata-rata nilai sampel. Standar deviasi dapat dengan mudah dihitung dengan hanya mengakar kuadratkan nilai varians. Jika titik data lebih rendah dari rata-rata maka semakin tinggi standar deviasinya. Untuk menghitung standar deviasi dapat menggunakan rumus berikut: $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$","title":"Standar Deviasi"},{"location":"deskriptif/#skewness","text":"Skewness ( kemiringan ) mengacu pada distorsi atau asimetri dalam kurva lonceng simetris, atau distribusi normal dalam suatu set data. Skewness merupakan bentuk ketidaksimetrisan suatu distribusi data. Skewness juga adaalah angka yang menujukkan ketidak miringan atau kemiringan suatu data. berikut adalah rumus untuk mencari skewness: $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Keterangan: Xi = titik data\\ x = rata-rata n = jumlah titik distribusi o = standar deviasi","title":"Skewness"},{"location":"deskriptif/#quartile","text":"Quartile adalah jenis Quantile. Quartile pertama (Q1) didefinisikan sebagai angka tengah antara angka terkecil dan median dari kumpulan data. Kuartil kedua (Q2) adalah median data. Kuartil ketiga (Q3) adalah nilai tengah antara median dan nilai tertinggi dari kumpulan data. Simpelnya, quantile ialah nilai yang dibagi 25%. berikut adalah rumus quantile: $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2= (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Keterangan: Q = nialai quarter n = banyak data","title":"Quartile"},{"location":"deskriptif/#penerapan-statistik-deskrtiptif-dalam-python","text":"","title":"Penerapan Statistik Deskrtiptif dalam Python"},{"location":"deskriptif/#alat-dan-bahan","text":"buatlah data dengan random di excel terleih dahulu, caranya dengan menggunakan formula =RANDBETWEEN(batas_bawah;batas_atas) . kemudian copast hasil tersebut sebagai values . Setelah itu save as .csv . kita menggunakan library python yakni, pandas dan scipy.","title":"Alat dan Bahan:"},{"location":"deskriptif/#langkah-langkah","text":"","title":"Langkah-langkah:"},{"location":"deskriptif/#pertama","text":"Mengimport library yang telah disiapkan tadi, yakni scipy dan pandas import pandas as pd from scipy import stats","title":"Pertama"},{"location":"deskriptif/#kedua","text":"Memuat data .csv yang telah dibuat df = pd . read_csv ( 'data_random.csv' , sep = ';' )","title":"Kedua"},{"location":"deskriptif/#ketiga","text":"Memuat penyimpanan data untuk disimpan kemudian untuk ditampilkan. Kemudian menghitung data yang diambil dari bebrapa kolom dari data file .csv dengan itersi, dan menghitung dengan cara yang telah disediakan di libary pandas. Kemudian visualisasikan data trsebut. data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes stats Tinggi Badan Berat Badan Usia Lingkar Badan Min 140 40 20 70 Max 190 70 50 100 Mean 164.882 54.72 34.832 85.228 Standard Deviasi 15.18 8.96 9.3 8.8 Variasi 230.35 80.27 86.4 77.42 Skewnes -0 0.1 0.08 -0.07 Quantile 1 151 47 27 78 Quantile 2 165 54 34 85 Quantile 3 179 63 43.25 93 Median 165 54 34 85 Modus 142 50 28 93","title":"Ketiga"},{"location":"deskriptif/#referensi","text":"https://id.wikipedia.org/wiki/Statistika_deskriptif https://www.investopedia.com/terms/s/skewness.asp https://rumusrumus.com/standar-deviasi/ https://statmat.id/pengertian-statistik-deskriptif-dan-statistik-inferensia/ https://www.asikbelajar.com/pengertian-modus-mode/ https://en.wikipedia.org/wiki/Median https://en.wikipedia.org/wiki/Mean","title":"Referensi"},{"location":"eliminasi_gauss/","text":"METODE ELIMINASI GAUSS \u00b6 Eliminasi Gauss ialah sebuah cara mengoperasikan nilai-nilai yang berada di dalam matriks sehingga dapat menjadi matriks yang lebih sederhana. Caranya ialah melakukan operasi baris sehingga matriks tersebut menjadi matriks yang eselon-baris. Ini dapat digunakan sebagai salah satu metode penyelesaian persamaan linear dengan menggunakan matriks Caranya dengan mengubah persamaan linear tersebut ke dalam matriks teraugmentasi dan mengoperasikannya. Setelah menjadi matriks Eselon-baris, lakukan substitusi balik untuk mendapatkan nilai dari variabel-variabel tersebut. Algoritma gauss jordan: Maka solusinya dapat dihitung dengan teknik penyulingan mundur ( backward substitution ): ELIMINASI GAUSS JORDAN \u00b6 \u00b6 Dalam aljabar linear, eliminasi Gauss-Jordan adalah versi dari eliminasi Gauss. Pada metode eliminasi Gauss-Jordan kita membuat nol elemen-elemen di bawah maupun di atas diagonal utama suatu matriks. Hasilnya adalah matriks tereduksi yang berupa matriks diagonal satuan (semua elemen pada diagonal utama bernilai 1, elemen-elemen lainnya nol). Dalam bentuk matriks, eliminasi Gauss-Jordan ditulis sebagai berikut: Listing program: import numpy as np #Definisi Matrix A = [] B = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) A . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) B . append ( h ) Matrix = np . array ( A , float ) Hasil = np . array ( B , float ) n = len ( Matrix ) #Eliminasi Gauss for k in range ( 0 , n - 1 ): for i in range ( k + 1 , n ): if Matrix [ i , k ] != 0 : lam = Matrix [ i , k ] / Matrix [ k , k ] Matrix [ i , k : n ] = Matrix [ i , k : n ] - ( Matrix [ k , k : n ] * lam ) Hasil [ i ] = Hasil [ i ] - ( Hasil [ k ] * lam ) print ( \"Matrix A : \" , ' \\n ' , Matrix ) #Subtitution x = np . zeros ( n , float ) for m in range ( n - 1 , - 1 , - 1 ): x [ m ] = ( Hasil [ m ] - np . dot ( Matrix [ m , m + 1 : n ], x [ m + 1 : n ])) / Matrix [ m , m ] print ( 'Nilai X ' , m + 1 , '=' , x [ m ]) output: Masukkan ukuran Matrix : 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Nilai : 1 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Nilai : 4 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Hasil : 12 Masukkan Hasil : 3 Masukkan Hasil : - 4 Matrix A : [[ 2. - 2. 5. ] [ 0. 6. - 0.5 ] [ 0. 0. - 7.25 ]] Nilai X 3 = 3.2413793103448274 Nilai X 2 = - 0.2298850574712644 Nilai X 1 = - 2.333333333333332 # Eliminasi Gauss Jacobi **Metode IterasiJacobi** merupakan salah satu bidang analisis numerik yang digunakan untuk menyelesaikan permasalahan Persamaan Linier dan sering dijumpai dalam berbagai disiplin ilmu. Metode Iterasi Jacobi merupakan salah satu metode tak langsung, yaitu bermula dari suatu hampiran penyelesaian awal dan kemudian berusaha memperbaiki hampiran dalam tak berhingga namun langkah konvergen. Metode Iterasi Jacobi ini digunakan untuk menyelesaikan persamaan Linier berukuran besar dan proporsi koefisien nolnya besar. ![img](https://raw.githubusercontent.com/lukmanarimashuri/lukmanarimashuri.github.io/master/jacobi.png) Listing program: from pprint import pprint from numpy import array , zeros , diag , diagflat , dot import numpy as np def jacobi ( A , b , N = 25 , x = None ): #Membuat iniial guess if x is None : x = zeros ( len ( A [ 0 ])) #Membuat vektor dari elemen matrix A D = diag ( A ) R = A - diagflat ( D ) #Iterasi for i in range ( N ): x = ( b - dot ( R , x )) / D return x Mat1 = [] Mat2 = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) Mat1 . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) Mat2 . append ( h ) A = array ( Mat1 , float ) b = array ( Mat2 , float ) x = len ( Mat1 ) guess = np . zeros ( x , float ) sol = jacobi ( A , b , N = 25 , x = guess ) print ( \"A:\" ) pprint ( A ) print ( \"b:\" ) pprint ( b ) print ( \"x:\" ) pprint ( sol ) Output: Masukkan ukuran Matrix : 3 Masukkan Nilai : 3 Masukkan Nilai : 1 Masukkan Nilai : - 1 Masukkan Nilai : 4 Masukkan Nilai : 7 Masukkan Nilai : - 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Hasil : 5 Masukkan Hasil : 20 Masukkan Hasil : 10 A : array ([[ 3. , 1. , - 1. ], [ 4. , 7. , - 3. ], [ 2. , - 2. , 5. ]]) b : array ([ 5. , 20. , 10. ]) x : array ([ 1.50602413 , 3.13253016 , 2.6506024 ]) **GAUSS SEIDEL** Listing Program dari persamaan 4x-y+z=7, 4x-8y+z=-21 dan -2x+y+5z=15. Iterasi yang digunakan sebanyak 100 iterasi sehingga dapat menghasilkan x=2,y=4 dan z=3 : def seidel ( a , x , b ): #Mencari Panjang Matrix n = len ( a ) for j in range ( 0 , n ): d = b [ j ] #Menghitung xi, yi, zi for i in range ( 0 , n ): if ( j != i ): d -= a [ j ][ i ] * x [ i ] x [ j ] = d / a [ j ][ j ] #Solusi return x m = int ( input ( \"Masukkan Panjang Matrix: \" )) a = [] b = [] for k in range ( m ): mat1 = [] for i in range ( m ): l = float ( input ( \"Masukkan a\" + str ( k + 1 ) + \",\" + str ( i + 1 ) + \": \" )) mat1 . append ( l ) h = float ( input ( \"Masukkan Hasil: \" )) b . append ( h ) a . append ( mat1 ) n = 3 x = [ 0 , 0 , 0 ] print ( x ) for i in range ( 0 , 100 ): x = seidel ( a , x , b ) print ( x ) Output: Masukkan Panjang Matrix : 3 Masukkan a1 , 1 : 4 Masukkan a1 , 2 : - 1 Masukkan a1 , 3 : 1 Masukkan Hasil : 7 Masukkan a2 , 1 : 4 Masukkan a2 , 2 : - 8 Masukkan a2 , 3 : 1 Masukkan Hasil : - 21 Masukkan a3 , 1 : - 2 Masukkan a3 , 2 : 1 Masukkan a3 , 3 : 5 Masukkan Hasil : 15 [ 0 , 0 , 0 ] [ 1.75 , 3.5 , 3.0 ] [ 1.875 , 3.9375 , 2.9625 ] [ 1.99375 , 3.9921875 , 2.9990625 ] [ 1.99828125 , 3.9990234375 , 2.9995078125 ] [ 1.99987890625 , 3.9998779296875 , 2.9999759765625003 ] [ 1.99997548828125 , 3.9999847412109375 , 2.999993247070312 ] [ 1.9999978735351562 , 3.9999980926513667 , 2.999999530883789 ] [ 1.9999996404418945 , 3.9999997615814205 , 2.9999999038604734 ] [ 1.9999999644302369 , 3.9999999701976776 , 2.9999999917325595 ] [ 1.9999999946162794 , 3.9999999962747097 , 2.99999999859157 ] [ 1.9999999994207849 , 3.9999999995343387 , 2.9999999998614464 ] [ 1.9999999999182232 , 3.999999999941793 , 2.999999999978931 ] [ 1.9999999999907154 , 3.999999999992724 , 2.9999999999977414 ] [ 1.9999999999987457 , 3.9999999999990905 , 2.9999999999996803 ] [ 1.9999999999998526 , 3.9999999999998863 , 2.9999999999999636 ] [ 1.9999999999999807 , 3.999999999999986 , 2.999999999999995 ] [ 1.9999999999999978 , 3.9999999999999987 , 2.9999999999999996 ] [ 1.9999999999999996 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ]","title":"Eliminasi Gauss"},{"location":"eliminasi_gauss/#metode-eliminasi-gauss","text":"Eliminasi Gauss ialah sebuah cara mengoperasikan nilai-nilai yang berada di dalam matriks sehingga dapat menjadi matriks yang lebih sederhana. Caranya ialah melakukan operasi baris sehingga matriks tersebut menjadi matriks yang eselon-baris. Ini dapat digunakan sebagai salah satu metode penyelesaian persamaan linear dengan menggunakan matriks Caranya dengan mengubah persamaan linear tersebut ke dalam matriks teraugmentasi dan mengoperasikannya. Setelah menjadi matriks Eselon-baris, lakukan substitusi balik untuk mendapatkan nilai dari variabel-variabel tersebut. Algoritma gauss jordan: Maka solusinya dapat dihitung dengan teknik penyulingan mundur ( backward substitution ):","title":"METODE ELIMINASI GAUSS"},{"location":"eliminasi_gauss/#eliminasi-gauss-jordan","text":"Dalam aljabar linear, eliminasi Gauss-Jordan adalah versi dari eliminasi Gauss. Pada metode eliminasi Gauss-Jordan kita membuat nol elemen-elemen di bawah maupun di atas diagonal utama suatu matriks. Hasilnya adalah matriks tereduksi yang berupa matriks diagonal satuan (semua elemen pada diagonal utama bernilai 1, elemen-elemen lainnya nol). Dalam bentuk matriks, eliminasi Gauss-Jordan ditulis sebagai berikut: Listing program: import numpy as np #Definisi Matrix A = [] B = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) A . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) B . append ( h ) Matrix = np . array ( A , float ) Hasil = np . array ( B , float ) n = len ( Matrix ) #Eliminasi Gauss for k in range ( 0 , n - 1 ): for i in range ( k + 1 , n ): if Matrix [ i , k ] != 0 : lam = Matrix [ i , k ] / Matrix [ k , k ] Matrix [ i , k : n ] = Matrix [ i , k : n ] - ( Matrix [ k , k : n ] * lam ) Hasil [ i ] = Hasil [ i ] - ( Hasil [ k ] * lam ) print ( \"Matrix A : \" , ' \\n ' , Matrix ) #Subtitution x = np . zeros ( n , float ) for m in range ( n - 1 , - 1 , - 1 ): x [ m ] = ( Hasil [ m ] - np . dot ( Matrix [ m , m + 1 : n ], x [ m + 1 : n ])) / Matrix [ m , m ] print ( 'Nilai X ' , m + 1 , '=' , x [ m ]) output: Masukkan ukuran Matrix : 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Nilai : 1 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Nilai : 4 Masukkan Nilai : 5 Masukkan Nilai : 2 Masukkan Hasil : 12 Masukkan Hasil : 3 Masukkan Hasil : - 4 Matrix A : [[ 2. - 2. 5. ] [ 0. 6. - 0.5 ] [ 0. 0. - 7.25 ]] Nilai X 3 = 3.2413793103448274 Nilai X 2 = - 0.2298850574712644 Nilai X 1 = - 2.333333333333332 # Eliminasi Gauss Jacobi **Metode IterasiJacobi** merupakan salah satu bidang analisis numerik yang digunakan untuk menyelesaikan permasalahan Persamaan Linier dan sering dijumpai dalam berbagai disiplin ilmu. Metode Iterasi Jacobi merupakan salah satu metode tak langsung, yaitu bermula dari suatu hampiran penyelesaian awal dan kemudian berusaha memperbaiki hampiran dalam tak berhingga namun langkah konvergen. Metode Iterasi Jacobi ini digunakan untuk menyelesaikan persamaan Linier berukuran besar dan proporsi koefisien nolnya besar. ![img](https://raw.githubusercontent.com/lukmanarimashuri/lukmanarimashuri.github.io/master/jacobi.png) Listing program: from pprint import pprint from numpy import array , zeros , diag , diagflat , dot import numpy as np def jacobi ( A , b , N = 25 , x = None ): #Membuat iniial guess if x is None : x = zeros ( len ( A [ 0 ])) #Membuat vektor dari elemen matrix A D = diag ( A ) R = A - diagflat ( D ) #Iterasi for i in range ( N ): x = ( b - dot ( R , x )) / D return x Mat1 = [] Mat2 = [] n = int ( input ( \"Masukkan ukuran Matrix: \" )) for i in range ( n ): baris = [] for i in range ( n ): a = int ( input ( \"Masukkan Nilai: \" )) baris . append ( a ) Mat1 . append ( baris ) for i in range ( n ): h = int ( input ( \"Masukkan Hasil: \" )) Mat2 . append ( h ) A = array ( Mat1 , float ) b = array ( Mat2 , float ) x = len ( Mat1 ) guess = np . zeros ( x , float ) sol = jacobi ( A , b , N = 25 , x = guess ) print ( \"A:\" ) pprint ( A ) print ( \"b:\" ) pprint ( b ) print ( \"x:\" ) pprint ( sol ) Output: Masukkan ukuran Matrix : 3 Masukkan Nilai : 3 Masukkan Nilai : 1 Masukkan Nilai : - 1 Masukkan Nilai : 4 Masukkan Nilai : 7 Masukkan Nilai : - 3 Masukkan Nilai : 2 Masukkan Nilai : - 2 Masukkan Nilai : 5 Masukkan Hasil : 5 Masukkan Hasil : 20 Masukkan Hasil : 10 A : array ([[ 3. , 1. , - 1. ], [ 4. , 7. , - 3. ], [ 2. , - 2. , 5. ]]) b : array ([ 5. , 20. , 10. ]) x : array ([ 1.50602413 , 3.13253016 , 2.6506024 ]) **GAUSS SEIDEL** Listing Program dari persamaan 4x-y+z=7, 4x-8y+z=-21 dan -2x+y+5z=15. Iterasi yang digunakan sebanyak 100 iterasi sehingga dapat menghasilkan x=2,y=4 dan z=3 : def seidel ( a , x , b ): #Mencari Panjang Matrix n = len ( a ) for j in range ( 0 , n ): d = b [ j ] #Menghitung xi, yi, zi for i in range ( 0 , n ): if ( j != i ): d -= a [ j ][ i ] * x [ i ] x [ j ] = d / a [ j ][ j ] #Solusi return x m = int ( input ( \"Masukkan Panjang Matrix: \" )) a = [] b = [] for k in range ( m ): mat1 = [] for i in range ( m ): l = float ( input ( \"Masukkan a\" + str ( k + 1 ) + \",\" + str ( i + 1 ) + \": \" )) mat1 . append ( l ) h = float ( input ( \"Masukkan Hasil: \" )) b . append ( h ) a . append ( mat1 ) n = 3 x = [ 0 , 0 , 0 ] print ( x ) for i in range ( 0 , 100 ): x = seidel ( a , x , b ) print ( x ) Output: Masukkan Panjang Matrix : 3 Masukkan a1 , 1 : 4 Masukkan a1 , 2 : - 1 Masukkan a1 , 3 : 1 Masukkan Hasil : 7 Masukkan a2 , 1 : 4 Masukkan a2 , 2 : - 8 Masukkan a2 , 3 : 1 Masukkan Hasil : - 21 Masukkan a3 , 1 : - 2 Masukkan a3 , 2 : 1 Masukkan a3 , 3 : 5 Masukkan Hasil : 15 [ 0 , 0 , 0 ] [ 1.75 , 3.5 , 3.0 ] [ 1.875 , 3.9375 , 2.9625 ] [ 1.99375 , 3.9921875 , 2.9990625 ] [ 1.99828125 , 3.9990234375 , 2.9995078125 ] [ 1.99987890625 , 3.9998779296875 , 2.9999759765625003 ] [ 1.99997548828125 , 3.9999847412109375 , 2.999993247070312 ] [ 1.9999978735351562 , 3.9999980926513667 , 2.999999530883789 ] [ 1.9999996404418945 , 3.9999997615814205 , 2.9999999038604734 ] [ 1.9999999644302369 , 3.9999999701976776 , 2.9999999917325595 ] [ 1.9999999946162794 , 3.9999999962747097 , 2.99999999859157 ] [ 1.9999999994207849 , 3.9999999995343387 , 2.9999999998614464 ] [ 1.9999999999182232 , 3.999999999941793 , 2.999999999978931 ] [ 1.9999999999907154 , 3.999999999992724 , 2.9999999999977414 ] [ 1.9999999999987457 , 3.9999999999990905 , 2.9999999999996803 ] [ 1.9999999999998526 , 3.9999999999998863 , 2.9999999999999636 ] [ 1.9999999999999807 , 3.999999999999986 , 2.999999999999995 ] [ 1.9999999999999978 , 3.9999999999999987 , 2.9999999999999996 ] [ 1.9999999999999996 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ] [ 2.0 , 4.0 , 3.0 ]","title":"ELIMINASI GAUSS JORDAN\u00b6"},{"location":"fuzzy/","text":"Fuzzy Clustering Pengertian Fuzzy Clustering Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu. berikut adalah algoritma dari FCM: <pre> https://docs.google.com/spreadsheets/d/1evBtIV5M6rlFC9bljB9d0laFhv3qqoo4/edit#gid=1325169538 </pre> berikut contoh code program fuzzy C-Means: \u200b```python from __future__ import division, print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = ['b', 'orange', 'g', 'r', 'c', 'm', 'y', 'k', 'Brown', 'ForestGreen'] centers = [[4, 2], [1, 7], [5, 6]] sigmas = [[0.8, 0.3], [0.3, 0.5], [1.1, 0.7]] np.random.seed(42) xpts = np.zeros(1) ypts = np.zeros(1) labels = np.zeros(1) for i, ((xmu, ymu), (xsigma, ysigma)) in enumerate(zip(centers, sigmas)): xpts = np.hstack((xpts, np.random.standard_normal(200) * xsigma + xmu)) ypts = np.hstack((ypts, np.random.standard_normal(200) * ysigma + ymu)) labels = np.hstack((labels, np.ones(200) * i)) fig0, ax0 = plt.subplots() for label in range(3): ax0.plot(xpts[labels == label], ypts[labels == label], '.', color=colors[label]) ax0.set_title('Test data: 200 points x3 clusters.') fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np.random.uniform(0, 1, (1100, 2)) * 10 u, u0, d, jm, p, fpc = fuzz.cluster.cmeans_predict( newdata.T, cntr, 2, error=0.005, maxiter=1000) cluster_membership = np.argmax(u, axis=0) fig3, ax3 = plt.subplots() ax3.set_title('Random points classifed according to known centers') for j in range(3): ax3.plot(newdata[cluster_membership == j, 0], newdata[cluster_membership == j, 1], 'o', label='series ' + str(j)) ax3.legend() plt.show()","title":"Fuzzy Clustering"},{"location":"metode_romberg/","text":"Metode Romberg \u00b6 Integrasi Romberg ialah teknik yang digunakan untuk menganalisis kasus fungsi yang diintegrasikan telah tersedia. Teknik ini memiliki keunggulan untuk menghasilkan nilai-nilai dari fungsi yang digunakan secara efisien bagi pengintegrasian secara numerik. Integrasi Romberg didasarkan pada ekstrapolasi Richardson, yaitu metode untuk mengkombinasikan dua perkiraan integral secara numerik untuk memperoleh nilai ketiga, yang lebih akurat, misal: O(h2N)O(h2N) --> O(h2N+2)O(h2N+2) Misal I(h) dan I(2h) dihitung dengan metode Trapesium dengan orde galat O(h2)O(h2), maka ekstrapolasi Richardson menghasilkan metode Simpson \u2153 dengan orde galat O(h4)O(h4). Kemudian jika I(h) dan I(2h)* dihitung dengan metode Simpson \u2153, maka akan menghasilkan kaidah Boole denganrorde galat O(h6)O(h6). Persamaan ekstrapolasi Richardson : J=I(h)+I(h)\u2212I(2h)2q\u22121J=I(h)+I(h)\u2212I(2h)2q\u22121 Misalkan I adalah nilai integrasi yang dinyatakan sebagai I=Ak+Ch2+Dh4+Eh6+...I=Ak+Ch2+Dh4+Eh6+... dalam hal ini h=(b\u2212a)nh=(b\u2212a)n dan AkAk merupakan nilai integrasi dengan metode Trapesium dengan jumlah pias n=2kn=2k dan Orde galatnya adalah O(h2)O(h2). A0,A1,...AkA0,A1,...Ak digunakan dalam persamaan ekstrapolasi Richardson untuk mendapatkan B1,B2,...,BkB1,B2,...,Bk, yaitu Bk=Ak+Ak\u2212Ak\u2212122\u22121Bk=Ak+Ak\u2212Ak\u2212122\u22121 Jadi, nilai I setelah diupdate adalah I=Bk+D\u2032h4+E\u2032h6+\u2026I=Bk+D\u2032h4+E\u2032h6+\u2026 dengan orde galat BkBk adalah O(h4)O(h4). Selanjutnya, menggunakan B1,B2,..,BkB1,B2,..,Bk pada persamaan ekstrapolasi Richardson untuk mendapatkan runtunan C2,C3,...,CkC2,C3,...,Ck, yaitu Ck=Bk+Bk\u2212Bk\u2212124\u22121Ck=Bk+Bk\u2212Bk\u2212124\u22121 nilai I saat ini adalah I=Ck+E\"h6+...I=Ck+E\"h6+... dengan orde galat CkCk adalah O(h6)O(h6). Demikian seterusnya. Dari runtunan diatas diperoleh tabel Romberg berikut: O ( h 2) Metode Trapesium O ( h 4) Metode Simpson O ( h 6) Metode Boole O ( h 8) Perbaikan ketiga O ( h 10) dst A 0 A 1 B 1 A 2 B 2 C 2 A 3 B 3 C 3 D3 A 4 B 4 C 4 D4 E4 E4 ialah nilai integrasi yang lebih baik. Listing Pemrograman \u00b6 Contoh 1 : Dalam contoh ini kita dapat melihat bahwa dengan menggunakan metode scipy.integrate.romberg() , kita bisa mendapatkan integrasi romberg dari fungsi yang dapat dipanggil dari batas a ke b dengan menggunakan metode scipy.integrate.romberg() . # import numpy and scipy.integrate import numpy as np from integrate import scipy gfg = lambda x : np . exp ( - x * * 2 ) # using scipy.integrate.romberg() geek = integrate . romberg ( gfg , 0 , 3 , show = True ) print ( geek ) Output: Romberg integrasi < function vectorize1 .. vfunc di 0x00000209C3641EA0 > dari [ 0 , 3 ] Langkah StepSize Hasil 1 3.000000 1.500185 2 1.500000 0 , 908191 0 , 710860 4 0.750000 0.886180 0.878843 0.890042 8 0.375000 0.886199 0.886206 0.886696 0.886643 16 0.187500 0.886205 0.886207 0.886207 0.886200 0.886198 32 0 , 093750 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 64 0.046875 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 128 0.023438 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 Hasil akhir adalah 0 , 8862073482595311 setelah 129 evaluasi fungsi . Contoh 2: # import numpy and scipy.integrate import numpy as np from integrate import scipy gfg = lambda x : np . exp ( - x * * 2 ) + 1 / np . sqrt ( np . pi ) # using scipy.integrate.romberg() geek = integrate . romberg ( gfg , 1 , 2 , show = True ) print ( geek ) Output: Integrasi Romberg dari < function vectorize1 .. vfunc at 0x00000209E1605400 > dari [ 1 , 2 ] Langkah StepSize Hasil 1 1 , 000000 0 , 757287 2 0 , 500000 0 , 713438 0 , 698822 4 0.250000 0.702909 0.699400 0.699438 8 0.125000 0.700310 0.699444 0.699447 0.699447 16 0 , 062500 0 , 699663 0 , 699447 0 , 699447 0 , 699447 0 , 699447 32 0.031250 0.699501 0.699447 0.699447 0.699447 0.699447 0.699447 Hasil akhir adalah 0 , 6994468414978009 setelah 33 evaluasi fungsi .","title":"Metode Romberg"},{"location":"metode_romberg/#metode-romberg","text":"Integrasi Romberg ialah teknik yang digunakan untuk menganalisis kasus fungsi yang diintegrasikan telah tersedia. Teknik ini memiliki keunggulan untuk menghasilkan nilai-nilai dari fungsi yang digunakan secara efisien bagi pengintegrasian secara numerik. Integrasi Romberg didasarkan pada ekstrapolasi Richardson, yaitu metode untuk mengkombinasikan dua perkiraan integral secara numerik untuk memperoleh nilai ketiga, yang lebih akurat, misal: O(h2N)O(h2N) --> O(h2N+2)O(h2N+2) Misal I(h) dan I(2h) dihitung dengan metode Trapesium dengan orde galat O(h2)O(h2), maka ekstrapolasi Richardson menghasilkan metode Simpson \u2153 dengan orde galat O(h4)O(h4). Kemudian jika I(h) dan I(2h)* dihitung dengan metode Simpson \u2153, maka akan menghasilkan kaidah Boole denganrorde galat O(h6)O(h6). Persamaan ekstrapolasi Richardson : J=I(h)+I(h)\u2212I(2h)2q\u22121J=I(h)+I(h)\u2212I(2h)2q\u22121 Misalkan I adalah nilai integrasi yang dinyatakan sebagai I=Ak+Ch2+Dh4+Eh6+...I=Ak+Ch2+Dh4+Eh6+... dalam hal ini h=(b\u2212a)nh=(b\u2212a)n dan AkAk merupakan nilai integrasi dengan metode Trapesium dengan jumlah pias n=2kn=2k dan Orde galatnya adalah O(h2)O(h2). A0,A1,...AkA0,A1,...Ak digunakan dalam persamaan ekstrapolasi Richardson untuk mendapatkan B1,B2,...,BkB1,B2,...,Bk, yaitu Bk=Ak+Ak\u2212Ak\u2212122\u22121Bk=Ak+Ak\u2212Ak\u2212122\u22121 Jadi, nilai I setelah diupdate adalah I=Bk+D\u2032h4+E\u2032h6+\u2026I=Bk+D\u2032h4+E\u2032h6+\u2026 dengan orde galat BkBk adalah O(h4)O(h4). Selanjutnya, menggunakan B1,B2,..,BkB1,B2,..,Bk pada persamaan ekstrapolasi Richardson untuk mendapatkan runtunan C2,C3,...,CkC2,C3,...,Ck, yaitu Ck=Bk+Bk\u2212Bk\u2212124\u22121Ck=Bk+Bk\u2212Bk\u2212124\u22121 nilai I saat ini adalah I=Ck+E\"h6+...I=Ck+E\"h6+... dengan orde galat CkCk adalah O(h6)O(h6). Demikian seterusnya. Dari runtunan diatas diperoleh tabel Romberg berikut: O ( h 2) Metode Trapesium O ( h 4) Metode Simpson O ( h 6) Metode Boole O ( h 8) Perbaikan ketiga O ( h 10) dst A 0 A 1 B 1 A 2 B 2 C 2 A 3 B 3 C 3 D3 A 4 B 4 C 4 D4 E4 E4 ialah nilai integrasi yang lebih baik.","title":"Metode Romberg"},{"location":"metode_romberg/#listing-pemrograman","text":"Contoh 1 : Dalam contoh ini kita dapat melihat bahwa dengan menggunakan metode scipy.integrate.romberg() , kita bisa mendapatkan integrasi romberg dari fungsi yang dapat dipanggil dari batas a ke b dengan menggunakan metode scipy.integrate.romberg() . # import numpy and scipy.integrate import numpy as np from integrate import scipy gfg = lambda x : np . exp ( - x * * 2 ) # using scipy.integrate.romberg() geek = integrate . romberg ( gfg , 0 , 3 , show = True ) print ( geek ) Output: Romberg integrasi < function vectorize1 .. vfunc di 0x00000209C3641EA0 > dari [ 0 , 3 ] Langkah StepSize Hasil 1 3.000000 1.500185 2 1.500000 0 , 908191 0 , 710860 4 0.750000 0.886180 0.878843 0.890042 8 0.375000 0.886199 0.886206 0.886696 0.886643 16 0.187500 0.886205 0.886207 0.886207 0.886200 0.886198 32 0 , 093750 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 64 0.046875 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 128 0.023438 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 0.886207 Hasil akhir adalah 0 , 8862073482595311 setelah 129 evaluasi fungsi . Contoh 2: # import numpy and scipy.integrate import numpy as np from integrate import scipy gfg = lambda x : np . exp ( - x * * 2 ) + 1 / np . sqrt ( np . pi ) # using scipy.integrate.romberg() geek = integrate . romberg ( gfg , 1 , 2 , show = True ) print ( geek ) Output: Integrasi Romberg dari < function vectorize1 .. vfunc at 0x00000209E1605400 > dari [ 1 , 2 ] Langkah StepSize Hasil 1 1 , 000000 0 , 757287 2 0 , 500000 0 , 713438 0 , 698822 4 0.250000 0.702909 0.699400 0.699438 8 0.125000 0.700310 0.699444 0.699447 0.699447 16 0 , 062500 0 , 699663 0 , 699447 0 , 699447 0 , 699447 0 , 699447 32 0.031250 0.699501 0.699447 0.699447 0.699447 0.699447 0.699447 Hasil akhir adalah 0 , 6994468414978009 setelah 33 evaluasi fungsi .","title":"Listing Pemrograman"},{"location":"missingvalue_knn/","text":"Missing Values dengan teknik K-NN \u00b6 Missing Value \u00b6 Missing value (data/nilai yang hilang) adalah suatu informasi yang tidak tersedia dalam suatu data. Missing value biasanya terjadi karena adanya suatu informasi dalam data tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Beberapa metode yang biasa digunakan untuk mencari data yang hilang tersebut, biasanya data diganti nilainya dengan nilai tengah atau dengan menyimpulkan dari nilai yang ada, dan atau munglin bahkan mengabaikan data yang hilang tersebut. Algoritma K-NN (K-Nearest Neighbors) \u00b6 K-NN adalah sebuah metode dimana metode ini melakukan klarifikasi berdasarkan data yang jaraknya paling dekat dengan data yang dicari. Mengatasi Missing Value dengan Metode K-NN pada Bahasa Pemrograman Python \u00b6 Untuk mempermudah dalam proses penyelesaiannya, dapat digunakan yakni library python, yakni pandas dan scipy. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 80 , np . nan , 65 ], 'Second Score' : [ 80 , 55 , 76 , np . nan ], 'Third Score' :[ np . nan , 60 , 90 , 87 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 80.0 0.0 1 80.0 55.0 60.0 2 0.0 76.0 90.0 3 65.0 0.0 87.0 Referensi \u00b6 https://www.dictio.id/t/apa-yang-dimaksud-dengan-data-hilang-atau-missing-data-pada-statistik/116500 https://openlibrary.telkomuniversity.ac.id/pustaka/files/114813/jurnal_eproc/imputasi-misssing-data-menggunakan-metode-k-nearest-neighbour-dengan-optimasi-algoritma-memetikamissing-value-imputation-using-k-nearest-neighbour-method-optimized-with-memetic-algorithm.pdf","title":"Missing Value with K Nearest Nei"},{"location":"missingvalue_knn/#missing-values-dengan-teknik-k-nn","text":"","title":"Missing Values dengan teknik K-NN"},{"location":"missingvalue_knn/#missing-value","text":"Missing value (data/nilai yang hilang) adalah suatu informasi yang tidak tersedia dalam suatu data. Missing value biasanya terjadi karena adanya suatu informasi dalam data tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Beberapa metode yang biasa digunakan untuk mencari data yang hilang tersebut, biasanya data diganti nilainya dengan nilai tengah atau dengan menyimpulkan dari nilai yang ada, dan atau munglin bahkan mengabaikan data yang hilang tersebut.","title":"Missing Value"},{"location":"missingvalue_knn/#algoritma-k-nn-k-nearest-neighbors","text":"K-NN adalah sebuah metode dimana metode ini melakukan klarifikasi berdasarkan data yang jaraknya paling dekat dengan data yang dicari.","title":"Algoritma K-NN (K-Nearest Neighbors)"},{"location":"missingvalue_knn/#mengatasi-missing-value-dengan-metode-k-nn-pada-bahasa-pemrograman-python","text":"Untuk mempermudah dalam proses penyelesaiannya, dapat digunakan yakni library python, yakni pandas dan scipy. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 80 , np . nan , 65 ], 'Second Score' : [ 80 , 55 , 76 , np . nan ], 'Third Score' :[ np . nan , 60 , 90 , 87 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Score Second Score Third Score 0 100.0 80.0 0.0 1 80.0 55.0 60.0 2 0.0 76.0 90.0 3 65.0 0.0 87.0","title":"Mengatasi Missing Value dengan Metode K-NN pada Bahasa Pemrograman Python"},{"location":"missingvalue_knn/#referensi","text":"https://www.dictio.id/t/apa-yang-dimaksud-dengan-data-hilang-atau-missing-data-pada-statistik/116500 https://openlibrary.telkomuniversity.ac.id/pustaka/files/114813/jurnal_eproc/imputasi-misssing-data-menggunakan-metode-k-nearest-neighbour-dengan-optimasi-algoritma-memetikamissing-value-imputation-using-k-nearest-neighbour-method-optimized-with-memetic-algorithm.pdf","title":"Referensi"},{"location":"regresi_linier/","text":"Regresi Linier Sederhana dan Berganda \u00b6 Regresi linier yaitu menentukan satu persaman dan garis yang menunjukkan hubungan antara variabel bebas dan variabel terikat, yang merupakan persamaan penduga yang berguna untuk menaksir/meramalkan variabel terikat. Untuk mempelajari hubungan-hubungan antara beberapa variabel, analisis ini terdiri dari dua bentuk, yaitu : Analisi Regresi Sederhana (simple analisis regresi) Analisi Regresi Berganda (multiple analisis regresi) Analisis regresi sederhana merupakan hubungan antara dua variabel yaitu variabel bebas (independent variable) dan variabel terikat (dependent variable). Sedangkan analisis regresi berganda merupakan hubungan antara 3 variabel atau lebih, yaitu sekurang-kurangnya 2 variabel bebas dengan satu variabel terikat. Regresi Linier Sederhana \u00b6 Regresi linier sederhana hanya ada satu peubah bebas X yang dihubungkan dengan satu peubah terikat Y. Bentuk model umum regresi sederhana yang menunjukkan antara dua variabel. Y = \ud835\udc4fx + a Dengan : Y = Variabel terikat x = Variabel bebas a = konstanta b = Parameter Koefisien Regresi Variabel Bebas Berikut adalah rumus untuk mencari nilai b: Berikut adalah rumus untuk mencari nilai a: Kemudian ada koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel lain. Berikut adalah rumus untuk menghitung koefisien korelasi: \" style=\"zoom:100%;\" /> Regresi Linier Berganda \u00b6 Regresi linier berganda hampir sama dengan Regresi linier sederhana, hanya saja pada Regresi linier berganda variabel penduga (variabel bebas) lebih dari satu variabel penduga. Tujuan analisis Regresi linier berganda adalah untuk mengukur intensitas hubungan antara dua variabel atau lebih dan memuat prediksi/perkiraan nilai Y atas nilai X. Bentuk persamaan Regresi linier berganda yang mencakup dua atau lebih variabel, yaitu : Dimana: y = variabel terikat. x = variable bebas. b = koefisien estimasi. a = konstanta Kali ini akan dilakukan perhitungan regresi linear berganda dengan dua fitur variabel bebas, berikut adalah contoh perhitungannya. X1 X2 Y 2 3 10 4 2 12 5 3 16 7 1 16 Memprediksi data dapat menggunakan persamaan y=x1b1+x2b2+ay=x1b1+x2b2+a tadi, namun sebelum menggunakan persamaan tersebut perlu untuk mencari koefisien estimasi dari b1, b2, dan a. No X1 X2 Y X1 ^ 2 X2 ^ 2 Y ^ 2 X1 * X2 X1 * Y X2 * Y 1 2 3 10 4 9 100 6 20 30 2 4 2 12 16 4 144 8 48 24 3 5 3 16 25 9 256 15 80 48 4 7 1 16 49 1 256 7 112 16 jumlah 18 9 54 94 23 756 36 260 118 data diatas kemudian dinormalisasi dengan cara berikut. Dari proses perhitungan tadi telah di dapatkan persamaan y=x1\u22172+x2\u22172+0y=x1\u22172+x2\u22172+0. misalkan ada data baru dengan x1=6x1=6 dan x2=2x2=2 maka y=6\u22172+2\u22172+0=16 Implementasi dengan Sklearn Python \u00b6 import pandas as pd from sklearn.linear_model import LinearRegression data = pd . read_csv ( 'data.csv' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () X = df . iloc [ 0 :, 0 : 4 ] . values y = df . iloc [ 0 :, 4 ] . values reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) LinearRegresion() yang merupakan libarary dari sklearn untuk melakukan prediksi menggunakan metode regresi linear. 1.0 a = reg . intercept_ a attribute intercept_ untuk mendapatkan nilai konstanta - 276.51245551601414 reg . coef_ attribut coef_ untuk mendapatkan nilai coefisien dari seluruh fitur. array ([ 31.5480427 , 27.83274021 , 33.40569395 , 49.24199288 ]) b1 = reg . coef_ [ 0 ] b2 = reg . coef_ [ 1 ] b3 = reg . coef_ [ 2 ] b4 = reg . coef_ [ 3 ] x1 = 4 x2 = 5 x3 = 2 x4 = 1 y = b1 * x1 + b2 * x2 + b3 * x3 + b4 * x4 + a y 104.89679715302498 reg . predict ( np . array ([[ 4 , 5 , 2 , 1 ]])) array ([ 104.89679715 ])","title":"Regresi Linier"},{"location":"regresi_linier/#regresi-linier-sederhana-dan-berganda","text":"Regresi linier yaitu menentukan satu persaman dan garis yang menunjukkan hubungan antara variabel bebas dan variabel terikat, yang merupakan persamaan penduga yang berguna untuk menaksir/meramalkan variabel terikat. Untuk mempelajari hubungan-hubungan antara beberapa variabel, analisis ini terdiri dari dua bentuk, yaitu : Analisi Regresi Sederhana (simple analisis regresi) Analisi Regresi Berganda (multiple analisis regresi) Analisis regresi sederhana merupakan hubungan antara dua variabel yaitu variabel bebas (independent variable) dan variabel terikat (dependent variable). Sedangkan analisis regresi berganda merupakan hubungan antara 3 variabel atau lebih, yaitu sekurang-kurangnya 2 variabel bebas dengan satu variabel terikat.","title":"Regresi Linier Sederhana dan Berganda"},{"location":"regresi_linier/#regresi-linier-sederhana","text":"Regresi linier sederhana hanya ada satu peubah bebas X yang dihubungkan dengan satu peubah terikat Y. Bentuk model umum regresi sederhana yang menunjukkan antara dua variabel. Y = \ud835\udc4fx + a Dengan : Y = Variabel terikat x = Variabel bebas a = konstanta b = Parameter Koefisien Regresi Variabel Bebas Berikut adalah rumus untuk mencari nilai b: Berikut adalah rumus untuk mencari nilai a: Kemudian ada koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel lain. Berikut adalah rumus untuk menghitung koefisien korelasi: \" style=\"zoom:100%;\" />","title":"Regresi Linier Sederhana"},{"location":"regresi_linier/#regresi-linier-berganda","text":"Regresi linier berganda hampir sama dengan Regresi linier sederhana, hanya saja pada Regresi linier berganda variabel penduga (variabel bebas) lebih dari satu variabel penduga. Tujuan analisis Regresi linier berganda adalah untuk mengukur intensitas hubungan antara dua variabel atau lebih dan memuat prediksi/perkiraan nilai Y atas nilai X. Bentuk persamaan Regresi linier berganda yang mencakup dua atau lebih variabel, yaitu : Dimana: y = variabel terikat. x = variable bebas. b = koefisien estimasi. a = konstanta Kali ini akan dilakukan perhitungan regresi linear berganda dengan dua fitur variabel bebas, berikut adalah contoh perhitungannya. X1 X2 Y 2 3 10 4 2 12 5 3 16 7 1 16 Memprediksi data dapat menggunakan persamaan y=x1b1+x2b2+ay=x1b1+x2b2+a tadi, namun sebelum menggunakan persamaan tersebut perlu untuk mencari koefisien estimasi dari b1, b2, dan a. No X1 X2 Y X1 ^ 2 X2 ^ 2 Y ^ 2 X1 * X2 X1 * Y X2 * Y 1 2 3 10 4 9 100 6 20 30 2 4 2 12 16 4 144 8 48 24 3 5 3 16 25 9 256 15 80 48 4 7 1 16 49 1 256 7 112 16 jumlah 18 9 54 94 23 756 36 260 118 data diatas kemudian dinormalisasi dengan cara berikut. Dari proses perhitungan tadi telah di dapatkan persamaan y=x1\u22172+x2\u22172+0y=x1\u22172+x2\u22172+0. misalkan ada data baru dengan x1=6x1=6 dan x2=2x2=2 maka y=6\u22172+2\u22172+0=16","title":"Regresi Linier Berganda"},{"location":"regresi_linier/#implementasi-dengan-sklearn-python","text":"import pandas as pd from sklearn.linear_model import LinearRegression data = pd . read_csv ( 'data.csv' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () X = df . iloc [ 0 :, 0 : 4 ] . values y = df . iloc [ 0 :, 4 ] . values reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) LinearRegresion() yang merupakan libarary dari sklearn untuk melakukan prediksi menggunakan metode regresi linear. 1.0 a = reg . intercept_ a attribute intercept_ untuk mendapatkan nilai konstanta - 276.51245551601414 reg . coef_ attribut coef_ untuk mendapatkan nilai coefisien dari seluruh fitur. array ([ 31.5480427 , 27.83274021 , 33.40569395 , 49.24199288 ]) b1 = reg . coef_ [ 0 ] b2 = reg . coef_ [ 1 ] b3 = reg . coef_ [ 2 ] b4 = reg . coef_ [ 3 ] x1 = 4 x2 = 5 x3 = 2 x4 = 1 y = b1 * x1 + b2 * x2 + b3 * x3 + b4 * x4 + a y 104.89679715302498 reg . predict ( np . array ([[ 4 , 5 , 2 , 1 ]])) array ([ 104.89679715 ])","title":"Implementasi dengan Sklearn Python"},{"location":"richardson/","text":"Richardson Extrapolation \u00b6 Dalam analisis numerik, ekstrapolasi Richardson adalah metode percepatan urutan, yang digunakan untuk meningkatkan laju konvergensi suatu urutan. Ekstrapolasi Richardson diterapkan pada barisan pendekatan nilai opsi untuk mempercepat laju konvergensinya. Aplikasi praktis ekstrapolasi Richardson termasuk integrasi Romberg, yang menerapkan ekstrapolasi Richardson pada aturan trapesium. Teori \u00b6 Dalam rumus : ( f (x + h) - f (x - h) ) / (2 h) untuk nilai h yang sangat kecil, dua fungsi evaluasi f (x + h) dan f (x - h) akan menjadi hampir sama, dan pembatalan subtraktif akan terjadi. Oleh karena itu, tidak disarankan untuk menggunakan nilai h yang semakin kecil. dapat dicoba untuk memperkirakan nilai tepat e dengan perkiraan a(h) , dalam hal ini e adalah turunan dari f (1) (x) dengan perkiraan ( h ) = (f (x + h) - f (x - h)) / (2 h) . Dimisalkan sekarang bahwa kesalahan aproksimasi didefinisikan oleh serangkaian bentuk Taylor : e = a(h) + K h n + o(h n ) Jika menggunakan h / 2 : e = a(h/2) + K (h/2)n + o((h/2)n) = a(h/2) + K/2n h n + o(h n ) Mengalikan kedua ekspresi dengan 2 n dan mengurangi hasil persamaan yang pertama: 2n e \u2212 e = 2na(h/2) \u2212 a(h) + K/2n h n \u2212 K h n + o(h n ) Memperhatikan bahwa istilah h n dibatalkan dan dibiarkan dengan: (2n \u2212 1)e = 2na(h/2) \u2212 a(h) + o(h n ) Jika seri Taylor lengkap, rumus perbedaan terpusat yang terpusat, kita perhatikan bahwa istilah kesalahannya dalam bentuk Knh n dapat kita tulis dengan: K1 = \u22121/6 f(3)(x)h 2 , etc. Contoh Program \u00b6 from math import * def zeros ( n , m ): Z = [] for i in range ( n ): Z . append ([ 0 ] * m ) return Z def D ( Func , a , h ): return ( Func ( a + h ) - Func ( a - h )) / ( 2 * h ) def Richardson_dif ( func , a ): '''Richardson extrapolation method for numerical calculation of first derivative ''' k = 9 L = zeros ( k , k ) for I in range ( k ): L [ I ][ 0 ] = D ( func , a , 1 / ( 2 ** ( I + 1 ))) for j in range ( 1 , k ): for i in range ( k - j ): L [ i ][ j ] = (( 4 ** ( j )) * L [ i + 1 ][ j - 1 ] - L [ i ][ j - 1 ]) / ( 4 ** ( j ) - 1 ) return L [ 0 ][ k - 1 ] print ( 'Diferensiasi numerik dari: f = -0.1*x**4-0.15*x**3-0.5*x**2-0.25*x+1.2 dengan x = 0.5' ) print ( \"=======================================================================\" ) print ( ' %04.20f ' % Richardson_dif ( lambda x : - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 , 0.5 )) print ( \"=======================================================================\" ) print ( 'diff(2**cos(pi+sin(x)) dengan x = pi/2 adalah = %04.20f ' % Richardson_dif ( lambda x : 2 ** cos ( pi + sin ( x )), pi / 3 )) Hasil Running \u00b6 Diferensiasi numerik dari : f = - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 dengan x = 0.5 ======================================================================= - 0.91250000000000530687 ======================================================================= diff ( 2 ** cos ( pi + sin ( x )) dengan x = pi / 2 adalah = 0.16849558398154249050 >>>","title":"Richardson Extrapolation"},{"location":"richardson/#richardson-extrapolation","text":"Dalam analisis numerik, ekstrapolasi Richardson adalah metode percepatan urutan, yang digunakan untuk meningkatkan laju konvergensi suatu urutan. Ekstrapolasi Richardson diterapkan pada barisan pendekatan nilai opsi untuk mempercepat laju konvergensinya. Aplikasi praktis ekstrapolasi Richardson termasuk integrasi Romberg, yang menerapkan ekstrapolasi Richardson pada aturan trapesium.","title":"Richardson Extrapolation"},{"location":"richardson/#teori","text":"Dalam rumus : ( f (x + h) - f (x - h) ) / (2 h) untuk nilai h yang sangat kecil, dua fungsi evaluasi f (x + h) dan f (x - h) akan menjadi hampir sama, dan pembatalan subtraktif akan terjadi. Oleh karena itu, tidak disarankan untuk menggunakan nilai h yang semakin kecil. dapat dicoba untuk memperkirakan nilai tepat e dengan perkiraan a(h) , dalam hal ini e adalah turunan dari f (1) (x) dengan perkiraan ( h ) = (f (x + h) - f (x - h)) / (2 h) . Dimisalkan sekarang bahwa kesalahan aproksimasi didefinisikan oleh serangkaian bentuk Taylor : e = a(h) + K h n + o(h n ) Jika menggunakan h / 2 : e = a(h/2) + K (h/2)n + o((h/2)n) = a(h/2) + K/2n h n + o(h n ) Mengalikan kedua ekspresi dengan 2 n dan mengurangi hasil persamaan yang pertama: 2n e \u2212 e = 2na(h/2) \u2212 a(h) + K/2n h n \u2212 K h n + o(h n ) Memperhatikan bahwa istilah h n dibatalkan dan dibiarkan dengan: (2n \u2212 1)e = 2na(h/2) \u2212 a(h) + o(h n ) Jika seri Taylor lengkap, rumus perbedaan terpusat yang terpusat, kita perhatikan bahwa istilah kesalahannya dalam bentuk Knh n dapat kita tulis dengan: K1 = \u22121/6 f(3)(x)h 2 , etc.","title":"Teori"},{"location":"richardson/#contoh-program","text":"from math import * def zeros ( n , m ): Z = [] for i in range ( n ): Z . append ([ 0 ] * m ) return Z def D ( Func , a , h ): return ( Func ( a + h ) - Func ( a - h )) / ( 2 * h ) def Richardson_dif ( func , a ): '''Richardson extrapolation method for numerical calculation of first derivative ''' k = 9 L = zeros ( k , k ) for I in range ( k ): L [ I ][ 0 ] = D ( func , a , 1 / ( 2 ** ( I + 1 ))) for j in range ( 1 , k ): for i in range ( k - j ): L [ i ][ j ] = (( 4 ** ( j )) * L [ i + 1 ][ j - 1 ] - L [ i ][ j - 1 ]) / ( 4 ** ( j ) - 1 ) return L [ 0 ][ k - 1 ] print ( 'Diferensiasi numerik dari: f = -0.1*x**4-0.15*x**3-0.5*x**2-0.25*x+1.2 dengan x = 0.5' ) print ( \"=======================================================================\" ) print ( ' %04.20f ' % Richardson_dif ( lambda x : - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 , 0.5 )) print ( \"=======================================================================\" ) print ( 'diff(2**cos(pi+sin(x)) dengan x = pi/2 adalah = %04.20f ' % Richardson_dif ( lambda x : 2 ** cos ( pi + sin ( x )), pi / 3 ))","title":"Contoh Program"},{"location":"richardson/#hasil-running","text":"Diferensiasi numerik dari : f = - 0.1 * x ** 4 - 0.15 * x ** 3 - 0.5 * x ** 2 - 0.25 * x + 1.2 dengan x = 0.5 ======================================================================= - 0.91250000000000530687 ======================================================================= diff ( 2 ** cos ( pi + sin ( x )) dengan x = pi / 2 adalah = 0.16849558398154249050 >>>","title":"Hasil Running"}]}